{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size : 101\r\n",
      "Prediction :  [0 3 4 3 4 4 4 4 4 5 4 1 4 1 4 3 0 2 2 3 4 5 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4\r\n",
      " 4 4 4 4 5 4]\r\n",
      "Actual value :  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5]\r\n",
      "Testing Validation Accuracy :  0.348837209302\r\n"
     ]
    }
   ],
   "source": [
    "!python trainingtest.py -ds copy.txt -ts textcopy.txt -at gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.naive_bayes in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.naive_bayes\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These\n",
      "    are supervised learning methods based on applying Bayes' theorem with strong\n",
      "    (naive) feature independence assumptions.\n",
      "\n",
      "CLASSES\n",
      "    BaseDiscreteNB(BaseNB)\n",
      "        BernoulliNB\n",
      "        MultinomialNB\n",
      "    BaseNB(abc.NewBase)\n",
      "        GaussianNB\n",
      "    \n",
      "    class BernoulliNB(BaseDiscreteNB)\n",
      "     |  Naive Bayes classifier for multivariate Bernoulli models.\n",
      "     |  \n",
      "     |  Like MultinomialNB, this classifier is suitable for discrete data. The\n",
      "     |  difference is that while MultinomialNB works with occurrence counts,\n",
      "     |  BernoulliNB is designed for binary/boolean features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional (default=1.0)\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  binarize : float or None, optional (default=0.0)\n",
      "     |      Threshold for binarizing (mapping to booleans) of sample features.\n",
      "     |      If None, input is presumed to already consist of binary vectors.\n",
      "     |  \n",
      "     |  fit_prior : boolean, optional (default=True)\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like, size=[n_classes,], optional (default=None)\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_log_prior_ : array, shape = [n_classes]\n",
      "     |      Log probability of each class (smoothed).\n",
      "     |  \n",
      "     |  feature_log_prob_ : array, shape = [n_classes, n_features]\n",
      "     |      Empirical log probability of features given a class, P(x_i|y).\n",
      "     |  \n",
      "     |  class_count_ : array, shape = [n_classes]\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  feature_count_ : array, shape = [n_classes, n_features]\n",
      "     |      Number of samples encountered for each (class, feature)\n",
      "     |      during fitting. This value is weighted by the sample weight when\n",
      "     |      provided.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.random.randint(2, size=(6, 100))\n",
      "     |  >>> Y = np.array([1, 2, 3, 4, 4, 5])\n",
      "     |  >>> from sklearn.naive_bayes import BernoulliNB\n",
      "     |  >>> clf = BernoulliNB()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      "     |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      "     |  http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
      "     |  \n",
      "     |  A. McCallum and K. Nigam (1998). A comparison of event models for naive\n",
      "     |  Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n",
      "     |  Text Categorization, pp. 41-48.\n",
      "     |  \n",
      "     |  V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n",
      "     |  naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BernoulliNB\n",
      "     |      BaseDiscreteNB\n",
      "     |      BaseNB\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], (default=None)\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like, shape = [n_classes] (default=None)\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] (default=None)\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class GaussianNB(BaseNB)\n",
      "     |  Gaussian Naive Bayes (GaussianNB)\n",
      "     |  \n",
      "     |  Can perform online updates to model parameters via `partial_fit` method.\n",
      "     |  For details on algorithm used to update feature means and variance online,\n",
      "     |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      "     |  \n",
      "     |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  priors : array-like, shape (n_classes,)\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_prior_ : array, shape (n_classes,)\n",
      "     |      probability of each class.\n",
      "     |  \n",
      "     |  class_count_ : array, shape (n_classes,)\n",
      "     |      number of training samples observed in each class.\n",
      "     |  \n",
      "     |  theta_ : array, shape (n_classes, n_features)\n",
      "     |      mean of each feature per class\n",
      "     |  \n",
      "     |  sigma_ : array, shape (n_classes, n_features)\n",
      "     |      variance of each feature per class\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> clf = GaussianNB()\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  GaussianNB(priors=None)\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  >>> clf_pf = GaussianNB()\n",
      "     |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      "     |  GaussianNB(priors=None)\n",
      "     |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GaussianNB\n",
      "     |      BaseNB\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, priors=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Gaussian Naive Bayes according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance and numerical stability overhead,\n",
      "     |      hence it is better to call partial_fit on chunks of data that are\n",
      "     |      as large as possible (as long as fitting in the memory budget) to\n",
      "     |      hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like, shape (n_classes,), optional (default=None)\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class MultinomialNB(BaseDiscreteNB)\n",
      "     |  Naive Bayes classifier for multinomial models\n",
      "     |  \n",
      "     |  The multinomial Naive Bayes classifier is suitable for classification with\n",
      "     |  discrete features (e.g., word counts for text classification). The\n",
      "     |  multinomial distribution normally requires integer feature counts. However,\n",
      "     |  in practice, fractional counts such as tf-idf may also work.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, optional (default=1.0)\n",
      "     |      Additive (Laplace/Lidstone) smoothing parameter\n",
      "     |      (0 for no smoothing).\n",
      "     |  \n",
      "     |  fit_prior : boolean, optional (default=True)\n",
      "     |      Whether to learn class prior probabilities or not.\n",
      "     |      If false, a uniform prior will be used.\n",
      "     |  \n",
      "     |  class_prior : array-like, size (n_classes,), optional (default=None)\n",
      "     |      Prior probabilities of the classes. If specified the priors are not\n",
      "     |      adjusted according to the data.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  class_log_prior_ : array, shape (n_classes, )\n",
      "     |      Smoothed empirical log probability for each class.\n",
      "     |  \n",
      "     |  intercept_ : property\n",
      "     |      Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |  feature_log_prob_ : array, shape (n_classes, n_features)\n",
      "     |      Empirical log probability of features\n",
      "     |      given a class, ``P(x_i|y)``.\n",
      "     |  \n",
      "     |  coef_ : property\n",
      "     |      Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
      "     |      as a linear model.\n",
      "     |  \n",
      "     |  class_count_ : array, shape (n_classes,)\n",
      "     |      Number of samples encountered for each class during fitting. This\n",
      "     |      value is weighted by the sample weight when provided.\n",
      "     |  \n",
      "     |  feature_count_ : array, shape (n_classes, n_features)\n",
      "     |      Number of samples encountered for each (class, feature)\n",
      "     |      during fitting. This value is weighted by the sample weight when\n",
      "     |      provided.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> X = np.random.randint(5, size=(6, 100))\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "     |  >>> clf = MultinomialNB()\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "     |  >>> print(clf.predict(X[2:3]))\n",
      "     |  [3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
      "     |  naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
      "     |  Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
      "     |  Information Retrieval. Cambridge University Press, pp. 234-265.\n",
      "     |  http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultinomialNB\n",
      "     |      BaseDiscreteNB\n",
      "     |      BaseNB\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, fit_prior=True, class_prior=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDiscreteNB:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Naive Bayes classifier according to X, y\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], (default=None)\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Incremental fit on a batch of samples.\n",
      "     |      \n",
      "     |      This method is expected to be called several times consecutively\n",
      "     |      on different chunks of a dataset so as to implement out-of-core\n",
      "     |      or online learning.\n",
      "     |      \n",
      "     |      This is especially useful when the whole dataset is too big to fit in\n",
      "     |      memory at once.\n",
      "     |      \n",
      "     |      This method has some performance overhead hence it is better to call\n",
      "     |      partial_fit on chunks of data that are as large as possible\n",
      "     |      (as long as fitting in the memory budget) to hide the overhead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      classes : array-like, shape = [n_classes] (default=None)\n",
      "     |          List of all the classes that can possibly appear in the y vector.\n",
      "     |      \n",
      "     |          Must be provided at the first call to partial_fit, can be omitted\n",
      "     |          in subsequent calls.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] (default=None)\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseDiscreteNB:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseNB:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Perform classification on an array of test vectors X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape = [n_samples]\n",
      "     |          Predicted target values for X\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Return log-probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the log-probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Return probability estimates for the test vector X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = [n_samples, n_features]\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array-like, shape = [n_samples, n_classes]\n",
      "     |          Returns the probability of the samples for each class in\n",
      "     |          the model. The columns correspond to the classes in sorted\n",
      "     |          order, as they appear in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB']\n",
      "\n",
      "FILE\n",
      "    /Users/neonexxa/machine_learning/lib/python3.6/site-packages/sklearn/naive_bayes.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"saxngram.py\", line 32\r\n",
      "    print(PATH)\r\n",
      "              ^\r\n",
      "TabError: inconsistent use of tabs and spaces in indentation\r\n"
     ]
    }
   ],
   "source": [
    "!python saxngram.py -t collection -y byyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recursivengram as rngm\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = ['ch','jb','kch','lgk','pj','sbg']\n",
    "locname = ['cam','johor baru','kucing','lgk','pj','sbg']\n",
    "datasettype = ['ra','rf','tem']\n",
    "datasetname = ['Rain amount','Rain Frequency','Temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "graminput = 2\n",
    "byyear = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Ngram by collection\n",
      "============================================================\n",
      "Rain amount\n",
      "CH\n",
      "/rawSAX/CH/rach.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "getfile() missing 1 required positional argument: 'vline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-23787ec24bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/rawSAX/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatasettype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrngm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraminput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbyyear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: getfile() missing 1 required positional argument: 'vline'"
     ]
    }
   ],
   "source": [
    "byyear = 1\n",
    "print (\"============================================================\")\n",
    "print (\"Ngram by collection\")\n",
    "for y in range(len(datasettype)):\n",
    "    print (\"============================================================\")\n",
    "    print (datasetname[y])\n",
    "    for x in range(len(loc)):\n",
    "        print (loc[x].upper())\n",
    "        PATH = '/rawSAX/'+loc[x].upper()+'/'+datasettype[y]+loc[x]+'.txt'\n",
    "        print(PATH)\n",
    "        rngm.getfile(PATH,graminput,byyear,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
